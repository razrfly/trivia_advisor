{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Database Schema Updates",
        "description": "Implement database changes to support duplicate venue detection and management",
        "details": "1. Add unique constraint on (name, postcode) combination in venues table\n2. Create audit_logs table for tracking merge operations with fields:\n   - id (PK)\n   - action_type (enum: 'merge', 'reject', etc.)\n   - primary_venue_id (FK to venues)\n   - secondary_venue_id (FK to venues)\n   - metadata (JSON)\n   - performed_by (user reference)\n   - timestamp\n3. Add soft delete functionality to venues table:\n   - is_deleted (boolean, default false)\n   - deleted_at (timestamp)\n   - deleted_by (user reference)\n   - merged_into_id (FK to venues, nullable)\n\nExample SQL:\n```sql\n-- Add unique constraint\nALTER TABLE venues ADD CONSTRAINT unique_name_postcode UNIQUE (name, postcode);\n\n-- Create audit log table\nCREATE TABLE venue_merge_logs (\n  id SERIAL PRIMARY KEY,\n  action_type VARCHAR(50) NOT NULL,\n  primary_venue_id INTEGER REFERENCES venues(id),\n  secondary_venue_id INTEGER REFERENCES venues(id),\n  metadata JSONB,\n  performed_by INTEGER REFERENCES users(id),\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add soft delete columns\nALTER TABLE venues \n  ADD COLUMN is_deleted BOOLEAN DEFAULT FALSE,\n  ADD COLUMN deleted_at TIMESTAMP,\n  ADD COLUMN deleted_by INTEGER REFERENCES users(id),\n  ADD COLUMN merged_into_id INTEGER REFERENCES venues(id);\n```",
        "testStrategy": "1. Unit tests for database migrations\n2. Verify unique constraint blocks duplicate entries\n3. Test soft delete functionality\n4. Ensure audit logs capture all required information\n5. Test rollback scenarios for merged venues\n6. Verify foreign key constraints work correctly",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add unique constraint on venue name and postcode",
            "description": "Implement a unique constraint on the venues table to prevent duplicate venues with the same name and postcode combination",
            "dependencies": [],
            "details": "Create a database migration that adds a unique constraint on the (name, postcode) combination in the venues table. Handle potential existing duplicates by identifying them first and creating a report for manual review before applying the constraint. Use ALTER TABLE statement with UNIQUE constraint.\n<info added on 2025-06-18T12:56:02.337Z>\n## Migration Analysis Results\n\n**DUPLICATES IDENTIFIED:**\n- 37 sets with same name+postcode (e.g., \"The Crown\" in SK6 2AA, \"BOXPARK Camden\" in NW1 8QP)\n- 15 sets with same name+city but no postcode (e.g., \"4 Noses Brewing Company - Broomfield\")\n\n**BLOCKING EXAMPLE:**\n- \"The Crown\" (SK6 2AA) in city 508: IDs [3015, 813] - Migration failed on this exact duplicate\n\n**REVISED APPROACH:**\n1. Modify migration to add constraints WITHOUT enforcing them initially\n2. Wait for duplicate resolution interface (Task 4) to manually merge the 52 identified duplicate sets\n3. Create a follow-up migration to enable constraint enforcement after duplicates are resolved\n\n**MIGRATION STATUS:**\n- Initial migration failed as expected due to existing duplicates\n- Duplicate detection logic successfully identified all 52 problem cases\n- Migration needs to be rolled back and modified to handle existing data\n</info added on 2025-06-18T12:56:02.337Z>\n<info added on 2025-06-18T12:57:59.698Z>\n## Migration Implementation Complete\n\n**INFRASTRUCTURE CREATED:**\n1. **First Migration (20250618125451)**: Preparation phase\n   - Created `venue_merge_logs` table for tracking merge operations\n   - Added soft delete columns to venues table (is_deleted, deleted_at, deleted_by, merged_into_id)\n   - Created `potential_duplicate_venues` view for easy duplicate identification\n   - Added performance indexes for all duplicate management queries\n\n2. **Second Migration (20250618125709)**: Enforcement phase (ready for future use)\n   - Unique constraint on (name, postcode) for venues with postcodes\n   - Unique constraint on (name, city_id) for venues without postcodes\n   - Both exclude soft-deleted venues (is_deleted = false)\n   - Built-in duplicate check that prevents migration if duplicates exist\n\n**CURRENT STATUS:**\n- 52 sets of duplicate venues identified and ready for resolution\n- Infrastructure in place to safely merge duplicates via Task 4 (Duplicate Review Interface)\n- Follow-up migration ready to enforce constraints once duplicates are resolved\n\n**VERIFICATION COMMANDS:**\n- View duplicates: `SELECT * FROM potential_duplicate_venues LIMIT 10;`\n- Check infrastructure: `\\d venue_merge_logs` and `\\d venues` in psql\n</info added on 2025-06-18T12:57:59.698Z>",
            "status": "done",
            "testStrategy": "Test with INSERT statements that attempt to create duplicate venues. Verify constraint violation errors are raised appropriately. Test edge cases like case sensitivity and whitespace handling."
          },
          {
            "id": 2,
            "title": "Create audit_logs table for tracking merge operations",
            "description": "Implement a new table to track all venue merge operations and related actions",
            "dependencies": [],
            "details": "Create a database migration that establishes the venue_merge_logs table with all required fields: id (PK), action_type (enum), primary_venue_id (FK), secondary_venue_id (FK), metadata (JSONB), performed_by (user reference), and created_at timestamp. Ensure proper foreign key constraints to the venues and users tables.",
            "status": "done",
            "testStrategy": "Verify table creation with sample INSERT statements. Test foreign key constraints by attempting to reference non-existent venues and users."
          },
          {
            "id": 3,
            "title": "Add soft delete functionality to venues table",
            "description": "Extend the venues table with columns needed for soft deletion and venue merging",
            "dependencies": [],
            "details": "Create a database migration that adds the following columns to the venues table: is_deleted (boolean with default false), deleted_at (timestamp), deleted_by (integer with foreign key to users), and merged_into_id (integer with foreign key to venues, allowing NULL). Include appropriate indexes on these columns to optimize queries that filter on deletion status.",
            "status": "done",
            "testStrategy": "Test the migration by performing soft deletes and verifying the columns are properly updated. Test the foreign key constraints with valid and invalid user and venue references."
          },
          {
            "id": 4,
            "title": "Implement database functions for venue merging",
            "description": "Create stored procedures or functions to handle the venue merging process",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Develop a database function that handles the venue merging process, including: 1) Marking the secondary venue as deleted, 2) Setting the merged_into_id to point to the primary venue, 3) Creating an audit log entry, and 4) Handling any related records that reference the secondary venue. The function should take primary_venue_id, secondary_venue_id, and user_id as parameters and return success/failure status.\n<info added on 2025-06-18T13:14:20.201Z>\n## Library Analysis: EctoSoftDelete vs Custom Implementation\n\nAfter evaluating the ecto_soft_delete library (v2.1.0), we should integrate it into our venue merging function for these reasons:\n\n1. The library provides automatic query filtering to prevent accidentally including deleted venues\n2. Uses timestamp-based deletion tracking (`deleted_at`) which is more informative than boolean flags\n3. Offers repo-level soft deletion methods that will simplify our implementation\n4. Well-maintained with active development\n\nImplementation changes needed:\n- Modify our venue merging function to use `Repo.soft_delete!/1` instead of manually setting `is_deleted = true`\n- Ensure our function still sets the `merged_into_id` and creates audit logs\n- Update function to use `deleted_at` timestamp field instead of boolean flag\n- Maintain compatibility with our existing venue merge tracking capabilities\n\nThis approach will provide more robust soft deletion while simplifying our code and reducing potential bugs in the merging process.\n</info added on 2025-06-18T13:14:20.201Z>\n<info added on 2025-06-18T13:20:46.940Z>\n## Implementation Complete: Ecto.SoftDelete Integration\n\nThe ecto_soft_delete library has been successfully integrated into our codebase:\n\n- Added dependency `{:ecto_soft_delete, \"~> 2.1\"}` to mix.exs\n- Updated Venue schema with `import Ecto.SoftDelete.Schema` and `soft_delete_schema()`\n- Enhanced TriviaAdvisor.Repo with `use Ecto.SoftDelete.Repo`\n- Migrated from boolean `is_deleted` column to timestamp-based `deleted_at`\n- Updated `potential_duplicate_venues` view to use `deleted_at IS NULL` convention\n- Verified all 4,135 venues preserved during migration\n- Confirmed view returns 55 duplicate sets as expected\n\nThe venue merging function now uses `TriviaAdvisor.Repo.soft_delete!(venue)` instead of manual deletion flags, providing automatic query filtering and more robust deletion tracking. All migrations and compilations completed successfully, and the system is ready for soft deletion operations in the venue merging process.\n</info added on 2025-06-18T13:20:46.940Z>",
            "status": "done",
            "testStrategy": "Create test cases with sample venues and related data. Verify that the merge function correctly updates all relevant records and creates appropriate audit logs. Test edge cases like merging already-merged venues."
          },
          {
            "id": 5,
            "title": "Create database views for duplicate venue detection",
            "description": "Implement database views to help identify potential duplicate venues",
            "dependencies": [
              1
            ],
            "details": "Create a database view that identifies potential duplicate venues based on similar names and matching or nearby postcodes. Use techniques like trigram similarity or soundex for name comparison. The view should include columns for both venue IDs, similarity score, and other relevant matching criteria to help administrators identify duplicates for potential merging.",
            "status": "done",
            "testStrategy": "Test the view with a dataset containing known duplicates and verify they are correctly identified. Analyze performance with larger datasets to ensure the view remains efficient."
          }
        ]
      },
      {
        "id": 2,
        "title": "VenueDuplicateDetector Service Implementation",
        "description": "Create a service for fuzzy matching of venue names and addresses to detect potential duplicates",
        "details": "Implement a service that provides fuzzy matching capabilities:\n\n1. Create a VenueDuplicateDetector class with methods:\n   - `findPotentialDuplicates(venue)`: Returns list of potential duplicates\n   - `calculateSimilarityScore(venue1, venue2)`: Returns similarity percentage\n   - `isDuplicate(venue1, venue2, threshold=0.85)`: Boolean check\n\n2. Implement name similarity using algorithms like:\n   - Levenshtein distance\n   - Jaro-Winkler\n   - n-gram similarity\n\n3. Address normalization and comparison:\n   - Strip common words (Street, Road, etc.)\n   - Normalize abbreviations\n   - Compare postcodes exactly\n\n4. Configurable thresholds:\n   - Name similarity threshold (default 85%)\n   - Address match requirements\n\nExample implementation:\n```python\nclass VenueDuplicateDetector:\n    def __init__(self, name_threshold=0.85):\n        self.name_threshold = name_threshold\n        \n    def normalize_name(self, name):\n        # Remove common words, lowercase, etc.\n        return normalized_name\n        \n    def normalize_address(self, address):\n        # Standardize format, abbreviations\n        return normalized_address\n        \n    def calculate_name_similarity(self, name1, name2):\n        # Use Levenshtein or other algorithm\n        normalized1 = self.normalize_name(name1)\n        normalized2 = self.normalize_name(name2)\n        return similarity_score\n        \n    def is_duplicate(self, venue1, venue2):\n        name_similarity = self.calculate_name_similarity(\n            venue1.name, venue2.name)\n            \n        if name_similarity >= self.name_threshold:\n            # Check address/postcode\n            if venue1.postcode == venue2.postcode:\n                return True\n                \n        # Check Google Place ID if available\n        if venue1.google_place_id and venue1.google_place_id == venue2.google_place_id:\n            return True\n            \n        return False\n        \n    def find_potential_duplicates(self, venue, all_venues):\n        return [v for v in all_venues if self.is_duplicate(venue, v)]\n```",
        "testStrategy": "1. Unit tests with known duplicate pairs\n2. Test with edge cases (similar names, different addresses)\n3. Benchmark performance with large venue datasets\n4. Test with real-world examples of known duplicates\n5. Validate false positive/negative rates\n6. Test normalization functions with various input formats",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Duplicate Review Interface",
        "description": "Create an admin interface for reviewing and managing suspected duplicate venues",
        "details": "Implement a dedicated admin page for reviewing and managing duplicate venues:\n\n1. Create DuplicateReviewController with endpoints:\n   - GET /admin/venues/duplicates - List suspected duplicates\n   - GET /admin/venues/duplicates/:id - View specific duplicate pair\n   - POST /admin/venues/duplicates/:id/merge - Merge venues\n   - POST /admin/venues/duplicates/:id/reject - Mark as not duplicate\n\n2. Implement UI components:\n   - Duplicate pairs list with filtering options\n   - Side-by-side comparison view of venue details\n   - Merge confirmation dialog with options\n   - Pagination for large result sets\n\n3. Display key venue information:\n   - Name, address, postcode\n   - Creation date and source\n   - Number of associated events\n   - Images and other metadata\n   - Similarity score and match criteria\n\n4. Sorting and filtering options:\n   - By similarity score\n   - By creation date\n   - By number of events\n   - By match criteria (name, address, Google Place ID)\n\nExample React component structure:\n```jsx\n// DuplicatesList.jsx\nfunction DuplicatesList() {\n  const [duplicates, setDuplicates] = useState([]);\n  const [filters, setFilters] = useState({ minSimilarity: 85 });\n  \n  useEffect(() => {\n    fetchDuplicates(filters);\n  }, [filters]);\n  \n  return (\n    <div className=\"duplicates-container\">\n      <FilterPanel filters={filters} onChange={setFilters} />\n      <Table>\n        {duplicates.map(pair => (\n          <DuplicateRow \n            key={pair.id}\n            venues={pair.venues}\n            similarity={pair.similarity}\n            onViewDetails={() => navigate(`/admin/venues/duplicates/${pair.id}`)}\n          />\n        ))}\n      </Table>\n      <Pagination />\n    </div>\n  );\n}\n\n// DuplicateDetails.jsx\nfunction DuplicateDetails({ pairId }) {\n  const [pair, setPair] = useState(null);\n  \n  const handleMerge = async () => {\n    // Call merge API\n  };\n  \n  const handleReject = async () => {\n    // Mark as not duplicate\n  };\n  \n  return pair ? (\n    <div className=\"comparison-view\">\n      <VenueComparisonPanel venues={pair.venues} />\n      <ActionButtons \n        onMerge={handleMerge}\n        onReject={handleReject}\n      />\n    </div>\n  ) : <Loading />;\n}\n```",
        "testStrategy": "1. Unit tests for controller endpoints\n2. Integration tests for the full review workflow\n3. UI component tests with mock data\n4. User acceptance testing with admin users\n5. Test pagination and filtering functionality\n6. Verify proper display of venue comparison data\n7. Test merge and reject functionality end-to-end",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "VenueMergeService Implementation",
        "description": "Create a service to safely merge duplicate venues while preserving associated data",
        "details": "Implement a service to handle the safe merging of duplicate venues:\n\n1. Create VenueMergeService with methods:\n   - `mergeVenues(primaryId, secondaryId, options)`: Main merge method\n   - `previewMerge(primaryId, secondaryId)`: Show what would happen\n   - `rollbackMerge(mergeLogId)`: Undo a previous merge\n\n2. Implement merge logic:\n   - Determine which venue to keep as primary (typically newer or with more events)\n   - Migrate all associated events to primary venue\n   - Combine metadata (prefer higher quality data)\n   - Soft-delete secondary venue with reference to primary\n   - Log all actions in audit table\n\n3. Handle edge cases:\n   - Conflicting data resolution\n   - Transaction management for atomicity\n   - Error handling and rollback support\n\nExample implementation:\n```python\nclass VenueMergeService:\n    def __init__(self, db, logger):\n        self.db = db\n        self.logger = logger\n        \n    def merge_venues(self, primary_id, secondary_id, options=None):\n        # Start transaction\n        with self.db.transaction():\n            try:\n                # Get venues\n                primary = self.db.venues.get(primary_id)\n                secondary = self.db.venues.get(secondary_id)\n                \n                if not primary or not secondary:\n                    raise ValueError(\"Venue not found\")\n                    \n                # Merge metadata\n                merged_data = self._merge_metadata(primary, secondary, options)\n                self.db.venues.update(primary_id, merged_data)\n                \n                # Migrate associated events\n                self._migrate_events(secondary_id, primary_id)\n                \n                # Soft-delete secondary venue\n                self.db.venues.update(secondary_id, {\n                    'is_deleted': True,\n                    'deleted_at': datetime.now(),\n                    'deleted_by': options.get('user_id'),\n                    'merged_into_id': primary_id\n                })\n                \n                # Log the merge\n                log_id = self.logger.log_merge(primary_id, secondary_id, options)\n                \n                return {\n                    'success': True,\n                    'primary_venue': primary_id,\n                    'log_id': log_id\n                }\n                \n            except Exception as e:\n                # Rollback happens automatically\n                self.logger.log_error(\"Merge failed\", str(e))\n                raise\n                \n    def _merge_metadata(self, primary, secondary, options):\n        # Logic to combine metadata, preferring newer/better data\n        result = primary.copy()\n        \n        # For each field, determine which value to keep\n        for field in ['description', 'phone', 'website', 'image_url']:\n            if self._is_better_data(secondary[field], primary[field]):\n                result[field] = secondary[field]\n                \n        return result\n        \n    def _migrate_events(self, from_id, to_id):\n        # Update all events to point to the primary venue\n        self.db.execute(\n            \"UPDATE events SET venue_id = ? WHERE venue_id = ?\",\n            [to_id, from_id]\n        )\n        \n    def rollback_merge(self, log_id):\n        # Implement rollback logic\n        # Restore soft-deleted venue\n        # Move events back\n        pass\n```",
        "testStrategy": "1. Unit tests for merge logic\n2. Integration tests with database\n3. Test rollback functionality\n4. Verify event migration works correctly\n5. Test edge cases (venues with many events, conflicting data)\n6. Verify audit logs are created correctly\n7. Performance testing with large datasets\n8. Test transaction isolation and atomicity",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "DuplicateAuditLogger Implementation",
        "description": "Create a logging service to track all duplicate detection and merge actions",
        "details": "Implement a logging service to track all duplicate-related actions:\n\n1. Create DuplicateAuditLogger class with methods:\n   - `logMerge(primaryId, secondaryId, metadata)`: Log merge action\n   - `logReject(venueId1, venueId2, reason)`: Log rejection of duplicate\n   - `logDetection(venueId1, venueId2, score)`: Log detection event\n   - `getAuditHistory(venueId)`: Get history for a venue\n\n2. Implement structured logging:\n   - Store all logs in the audit_logs table\n   - Include timestamp, user, action type, and relevant IDs\n   - Store additional metadata as JSON\n\n3. Add reporting capabilities:\n   - Summary statistics (merges per day/week)\n   - User activity reports\n   - Detection accuracy metrics\n\nExample implementation:\n```python\nclass DuplicateAuditLogger:\n    def __init__(self, db):\n        self.db = db\n        \n    def log_merge(self, primary_id, secondary_id, metadata=None):\n        return self._log_action({\n            'action_type': 'merge',\n            'primary_venue_id': primary_id,\n            'secondary_venue_id': secondary_id,\n            'metadata': metadata or {},\n            'performed_by': metadata.get('user_id') if metadata else None\n        })\n        \n    def log_reject(self, venue_id1, venue_id2, reason=None, user_id=None):\n        return self._log_action({\n            'action_type': 'reject',\n            'primary_venue_id': venue_id1,\n            'secondary_venue_id': venue_id2,\n            'metadata': {'reason': reason} if reason else {},\n            'performed_by': user_id\n        })\n        \n    def log_detection(self, venue_id1, venue_id2, similarity_score, criteria=None):\n        return self._log_action({\n            'action_type': 'detection',\n            'primary_venue_id': venue_id1,\n            'secondary_venue_id': venue_id2,\n            'metadata': {\n                'similarity_score': similarity_score,\n                'match_criteria': criteria or []\n            }\n        })\n        \n    def _log_action(self, data):\n        # Insert into audit_logs table\n        return self.db.venue_merge_logs.insert(data)\n        \n    def get_audit_history(self, venue_id):\n        # Query logs for this venue\n        return self.db.venue_merge_logs.find({\n            '$or': [\n                {'primary_venue_id': venue_id},\n                {'secondary_venue_id': venue_id}\n            ]\n        }).order_by('created_at', 'desc')\n        \n    def get_statistics(self, start_date=None, end_date=None):\n        # Generate statistics about merge operations\n        query = {}\n        if start_date:\n            query['created_at'] = {'$gte': start_date}\n        if end_date:\n            query.setdefault('created_at', {})['$lte'] = end_date\n            \n        results = self.db.venue_merge_logs.aggregate([\n            {'$match': query},\n            {'$group': {\n                '_id': {'action': '$action_type', 'day': {'$dateToString': {'format': '%Y-%m-%d', 'date': '$created_at'}}},\n                'count': {'$sum': 1}\n            }},\n            {'$sort': {'_id.day': 1}}\n        ])\n        \n        return results\n```",
        "testStrategy": "1. Unit tests for each logging method\n2. Verify correct data is stored in the database\n3. Test query methods return expected results\n4. Test statistics generation\n5. Verify performance with high volume of logs\n6. Test edge cases (missing data, large metadata objects)\n7. Integration tests with merge service",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Integrate Duplicate Detection with Scrapers",
        "description": "Extend scraper workflows to use duplicate detection before creating new venues",
        "details": "Integrate the duplicate detection service with venue scrapers to prevent creation of new duplicates:\n\n1. Modify scraper workflow:\n   - Before creating a new venue, check for potential duplicates\n   - If high-confidence match found, use existing venue\n   - If medium-confidence match, flag for review\n   - Log all decisions for later analysis\n\n2. Implementation steps:\n   - Create ScraperDuplicateHandler class\n   - Inject VenueDuplicateDetector service\n   - Add configuration for confidence thresholds\n   - Implement decision logic for different confidence levels\n\n3. Handle edge cases:\n   - Incomplete venue data from scrapers\n   - Multiple potential matches\n   - Conflicting information\n\nExample implementation:\n```python\nclass ScraperDuplicateHandler:\n    def __init__(self, duplicate_detector, venue_repo, logger, config=None):\n        self.detector = duplicate_detector\n        self.venue_repo = venue_repo\n        self.logger = logger\n        self.config = config or {\n            'high_confidence': 0.95,  # Auto-use existing venue\n            'medium_confidence': 0.85  # Flag for review\n        }\n        \n    async def process_venue(self, venue_data):\n        # Check if this might be a duplicate\n        potential_matches = await self.find_potential_duplicates(venue_data)\n        \n        if not potential_matches:\n            # No duplicates, create new venue\n            return await self.create_new_venue(venue_data)\n            \n        # Find best match\n        best_match = max(potential_matches, key=lambda m: m['score'])\n        \n        if best_match['score'] >= self.config['high_confidence']:\n            # High confidence - use existing venue\n            self.logger.log_detection(\n                best_match['venue'].id, \n                None,  # No ID for new venue yet\n                best_match['score'],\n                best_match['criteria']\n            )\n            return {\n                'action': 'use_existing',\n                'venue_id': best_match['venue'].id,\n                'confidence': best_match['score']\n            }\n            \n        elif best_match['score'] >= self.config['medium_confidence']:\n            # Medium confidence - create but flag for review\n            new_venue = await self.create_new_venue(venue_data)\n            \n            # Flag for review\n            await self.flag_for_review(new_venue.id, best_match['venue'].id, best_match['score'])\n            \n            return {\n                'action': 'flagged',\n                'venue_id': new_venue.id,\n                'potential_duplicate': best_match['venue'].id,\n                'confidence': best_match['score']\n            }\n            \n        else:\n            # Low confidence - create new venue\n            return await self.create_new_venue(venue_data)\n            \n    async def find_potential_duplicates(self, venue_data):\n        # Create temporary venue object for comparison\n        temp_venue = Venue(**venue_data)\n        \n        # Find potential duplicates\n        existing_venues = await self.venue_repo.find_all()\n        return self.detector.find_potential_duplicates(temp_venue, existing_venues)\n        \n    async def create_new_venue(self, venue_data):\n        # Create new venue in database\n        return await self.venue_repo.create(venue_data)\n        \n    async def flag_for_review(self, new_id, existing_id, score):\n        # Add to review queue\n        await self.venue_repo.add_to_review_queue({\n            'venue1_id': new_id,\n            'venue2_id': existing_id,\n            'similarity': score,\n            'status': 'pending',\n            'created_at': datetime.now()\n        })\n        \n        # Log the detection\n        self.logger.log_detection(new_id, existing_id, score)\n```",
        "testStrategy": "1. Unit tests for duplicate detection in scraper workflow\n2. Test with various confidence levels\n3. Integration tests with actual scraper code\n4. Test handling of edge cases (incomplete data, multiple matches)\n5. Verify correct logging of decisions\n6. Test performance impact on scraper operations\n7. Verify flagging for review works correctly",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Admin Venue Creation Validation",
        "description": "Add duplicate detection to the admin interface for manual venue creation",
        "details": "Enhance the admin interface for venue creation to detect and prevent duplicates:\n\n1. Modify venue creation form:\n   - Add real-time duplicate checking as fields are filled\n   - Show warnings for potential duplicates\n   - Allow admin to view and select existing venue instead\n\n2. Implementation steps:\n   - Create VenueValidationController with endpoints:\n     - POST /api/venues/validate - Check if venue might be duplicate\n   - Add client-side validation in venue form\n   - Implement UI components to show potential matches\n\n3. User experience considerations:\n   - Non-blocking warnings (allow override)\n   - Clear explanation of match criteria\n   - Easy way to view existing venue details\n\nExample implementation:\n```javascript\n// Server-side controller\nclass VenueValidationController {\n  async validate(req, res) {\n    const venueData = req.body;\n    const duplicateDetector = new VenueDuplicateDetector();\n    \n    // Check for potential duplicates\n    const potentialDuplicates = await duplicateDetector.findPotentialDuplicates(venueData);\n    \n    return res.json({\n      hasPotentialDuplicates: potentialDuplicates.length > 0,\n      potentialDuplicates: potentialDuplicates.map(d => ({\n        id: d.venue.id,\n        name: d.venue.name,\n        address: d.venue.address,\n        similarity: d.score,\n        matchCriteria: d.criteria\n      }))\n    });\n  }\n}\n\n// Client-side React component\nfunction VenueForm() {\n  const [formData, setFormData] = useState({});\n  const [duplicates, setDuplicates] = useState([]);\n  const [isChecking, setIsChecking] = useState(false);\n  \n  // Debounced validation function\n  const checkForDuplicates = useCallback(\n    debounce(async (data) => {\n      if (!data.name || !data.postcode) return;\n      \n      setIsChecking(true);\n      try {\n        const response = await api.post('/api/venues/validate', data);\n        setDuplicates(response.data.potentialDuplicates);\n      } catch (error) {\n        console.error('Validation error:', error);\n      } finally {\n        setIsChecking(false);\n      }\n    }, 500),\n    []\n  );\n  \n  // Check on form changes\n  useEffect(() => {\n    if (formData.name && formData.postcode) {\n      checkForDuplicates(formData);\n    }\n  }, [formData.name, formData.address, formData.postcode]);\n  \n  const handleChange = (e) => {\n    const { name, value } = e.target;\n    setFormData(prev => ({ ...prev, [name]: value }));\n  };\n  \n  return (\n    <form onSubmit={handleSubmit}>\n      <input \n        name=\"name\" \n        value={formData.name || ''} \n        onChange={handleChange} \n        placeholder=\"Venue name\"\n      />\n      \n      {/* Other form fields */}\n      \n      {duplicates.length > 0 && (\n        <div className=\"duplicate-warning\">\n          <h4>Potential duplicates found:</h4>\n          <ul>\n            {duplicates.map(d => (\n              <li key={d.id}>\n                <strong>{d.name}</strong> ({d.similarity.toFixed(2)}%)\n                <p>{d.address}</p>\n                <button \n                  type=\"button\" \n                  onClick={() => navigateToVenue(d.id)}\n                >\n                  View venue\n                </button>\n                <button \n                  type=\"button\" \n                  onClick={() => useExistingVenue(d.id)}\n                >\n                  Use this venue\n                </button>\n              </li>\n            ))}\n          </ul>\n        </div>\n      )}\n      \n      <button type=\"submit\" disabled={isChecking}>\n        {isChecking ? 'Checking...' : 'Create Venue'}\n      </button>\n    </form>\n  );\n}\n```",
        "testStrategy": "1. Unit tests for validation controller\n2. UI component tests for duplicate warnings\n3. Integration tests for the full form submission flow\n4. Test real-time validation with various input scenarios\n5. Verify correct display of potential matches\n6. Test user flow for selecting existing venue\n7. Test form submission with and without duplicates\n8. Verify performance of real-time validation",
        "priority": "low",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Monitoring and Reporting Dashboard",
        "description": "Create a dashboard to monitor duplicate detection performance and cleanup progress",
        "details": "Implement a monitoring dashboard to track the effectiveness of the duplicate management system:\n\n1. Create DuplicateMetricsController with endpoints:\n   - GET /api/metrics/duplicates/summary - Overall statistics\n   - GET /api/metrics/duplicates/trends - Time-based trends\n   - GET /api/metrics/duplicates/accuracy - Detection accuracy\n\n2. Implement dashboard components:\n   - Summary statistics (total duplicates, resolved, pending)\n   - Charts showing trends over time\n   - Detection accuracy metrics\n   - User activity reports\n\n3. Data collection and analysis:\n   - Calculate false positive/negative rates\n   - Track merge success rates\n   - Monitor user actions and efficiency\n   - Identify common duplicate patterns\n\nExample implementation:\n```javascript\n// Server-side controller\nclass DuplicateMetricsController {\n  async getSummary(req, res) {\n    const stats = await this.duplicateService.getStatistics();\n    return res.json({\n      totalDuplicatesDetected: stats.totalDetected,\n      duplicatesMerged: stats.merged,\n      duplicatesRejected: stats.rejected,\n      pendingReview: stats.pending,\n      averageSimilarityScore: stats.avgSimilarity,\n      falsePositiveRate: stats.falsePositiveRate\n    });\n  }\n  \n  async getTrends(req, res) {\n    const { period = 'day', start, end } = req.query;\n    const trends = await this.duplicateService.getTrends(period, start, end);\n    return res.json(trends);\n  }\n  \n  async getAccuracy(req, res) {\n    const accuracy = await this.duplicateService.getAccuracyMetrics();\n    return res.json({\n      truePositives: accuracy.truePositives,\n      falsePositives: accuracy.falsePositives,\n      falseNegatives: accuracy.falseNegatives,\n      precision: accuracy.precision,\n      recall: accuracy.recall,\n      f1Score: accuracy.f1Score\n    });\n  }\n}\n\n// Client-side React dashboard\nfunction DuplicatesDashboard() {\n  const [summary, setSummary] = useState(null);\n  const [trends, setTrends] = useState([]);\n  const [accuracy, setAccuracy] = useState(null);\n  const [dateRange, setDateRange] = useState({\n    start: subDays(new Date(), 30),\n    end: new Date()\n  });\n  \n  useEffect(() => {\n    // Fetch data\n    fetchSummary();\n    fetchTrends(dateRange.start, dateRange.end);\n    fetchAccuracy();\n  }, [dateRange]);\n  \n  return (\n    <div className=\"dashboard-container\">\n      <h1>Duplicate Venues Management Dashboard</h1>\n      \n      <div className=\"date-range-picker\">\n        <DateRangePicker \n          value={dateRange}\n          onChange={setDateRange}\n        />\n      </div>\n      \n      {summary && (\n        <div className=\"summary-cards\">\n          <MetricCard \n            title=\"Total Duplicates\" \n            value={summary.totalDuplicatesDetected} \n          />\n          <MetricCard \n            title=\"Merged\" \n            value={summary.duplicatesMerged} \n          />\n          <MetricCard \n            title=\"Rejected\" \n            value={summary.duplicatesRejected} \n          />\n          <MetricCard \n            title=\"Pending Review\" \n            value={summary.pendingReview} \n          />\n          <MetricCard \n            title=\"False Positive Rate\" \n            value={`${(summary.falsePositiveRate * 100).toFixed(1)}%`} \n          />\n        </div>\n      )}\n      \n      <div className=\"charts-container\">\n        <div className=\"chart\">\n          <h3>Duplicate Detection Trends</h3>\n          <LineChart data={trends} />\n        </div>\n        \n        {accuracy && (\n          <div className=\"chart\">\n            <h3>Detection Accuracy</h3>\n            <AccuracyChart data={accuracy} />\n          </div>\n        )}\n      </div>\n      \n      <div className=\"activity-log\">\n        <h3>Recent Activity</h3>\n        <ActivityTable />\n      </div>\n    </div>\n  );\n}\n```",
        "testStrategy": "1. Unit tests for metrics controller endpoints\n2. Test data aggregation and calculation logic\n3. UI component tests for dashboard elements\n4. Test with various date ranges and filters\n5. Verify charts display correct data\n6. Test dashboard performance with large datasets\n7. Verify real-time updates of metrics\n8. Test export functionality for reports",
        "priority": "low",
        "dependencies": [
          3,
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-18T12:53:23.245Z",
      "updated": "2025-06-18T13:20:58.080Z",
      "description": "Tasks for master context"
    }
  }
}